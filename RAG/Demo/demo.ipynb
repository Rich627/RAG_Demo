{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mac 用戶請先執行以下指令安裝 cmake\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 這個Notebook參考了這篇文章\n",
    "https://medium.com/@cch.chichieh/rag%E5%AF%A6%E4%BD%9C%E6%95%99%E5%AD%B8-langchain-llama2-%E5%89%B5%E9%80%A0%E4%BD%A0%E7%9A%84%E5%80%8B%E4%BA%BAllm-d6838febf8c4\n",
    "#### 下面是Code\n",
    "https://github.com/wsxqaza12/RAG_example/tree/master\n",
    "#### 其他相關文章\n",
    "https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain 支援很多種rag的方式, 我這邊用pdf\n",
    "loader = PyMuPDFLoader(\"Classmate.pdf\")\n",
    "PDF_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊是用splitter把text切成一些小塊的chunk, 這樣LLM的token才不會爆掉\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=5)\n",
    "all_splits = text_splitter.split_documents(PDF_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'db'\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                  model_kwargs=model_kwargs)\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from llama_cpp import Llama\n",
    "\n",
    "model = Llama.from_pretrained(\n",
    "    repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "    filename=\"llama-2-7b-chat.Q2_K.gguf\",\n",
    "    local_dir=\"model/\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "model_path = \"model/llama-2-7b-chat.Q2_K.gguf\"\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='<<SYS>> \\n    You are a helpful assistant eager to assist with providing better Google search results.\\n    <</SYS>> \\n    \\n    [INST] Provide an answer to the following question in 150 words. Ensure that the answer is informative,             relevant, and concise:\\n            {question} \\n    [/INST]')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 給LLM一些deafult的prompt相當於做一些設定\n",
    "DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<<SYS>> \n",
    "    You are a helpful assistant eager to assist with providing better Google search results.\n",
    "    <</SYS>> \n",
    "    \n",
    "    [INST] Provide an answer to the following question in 150 words. Ensure that the answer is informative, \\\n",
    "            relevant, and concise:\n",
    "            {question} \n",
    "    [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are a helpful assistant eager to assist with providing better Google search results. \\\n",
    "        Provide an answer to the following question in about 150 words. Ensure that the answer is informative, \\\n",
    "        relevant, and concise: \\\n",
    "        {question}\"\"\",\n",
    ")\n",
    "\n",
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=DEFAULT_SEARCH_PROMPT,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Higher-level machine learning (高等機會學習) refers to the application of machine learning techniques to solve complex problems that require a deeper understanding of the data and the underlying patterns. It involves using advanced algorithms and statistical models to extract meaningful insights from large and complex datasets, often leveraging domain knowledge and expertise from various fields. The goal of higher-level machine learning is to uncover hidden patterns and relationships in data, leading to more accurate predictions and decisions. Some examples of higher-level machine learning applications include image recognition, natural language processing, and recommender systems. These applications require a more sophisticated understanding of machine learning techniques and domain-specific knowledge, making them more challenging and rewarding than traditional machine learning tasks."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '高等機器學習是什麼?',\n",
       " 'text': '  Higher-level machine learning (高等機會學習) refers to the application of machine learning techniques to solve complex problems that require a deeper understanding of the data and the underlying patterns. It involves using advanced algorithms and statistical models to extract meaningful insights from large and complex datasets, often leveraging domain knowledge and expertise from various fields. The goal of higher-level machine learning is to uncover hidden patterns and relationships in data, leading to more accurate predictions and decisions. Some examples of higher-level machine learning applications include image recognition, natural language processing, and recommender systems. These applications require a more sophisticated understanding of machine learning techniques and domain-specific knowledge, making them more challenging and rewarding than traditional machine learning tasks.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 這邊測試一下LLM\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"高等機器學習是什麼?\"\n",
    "llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊就是我們論文講的retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 劉睿麒 is a student at Fu Jen Catholic University in Taiwan. His email address is 劉睿麒409170581@m365.fju.edu.tw. He is a member of the student body and has access to various university resources and services, such as the library, student clubs, and student government. He may also have a student email account, which is a standard email address provided by the university for official communication with students.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me about 劉睿麒?應該有一些關於他的資料吧?例如email之類的',\n",
       " 'result': ' 劉睿麒 is a student at Fu Jen Catholic University in Taiwan. His email address is 劉睿麒409170581@m365.fju.edu.tw. He is a member of the student body and has access to various university resources and services, such as the library, student clubs, and student government. He may also have a student email account, which is a standard email address provided by the university for official communication with students.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我們可以用這個retriever來問問題接著他就會去DB裡面找答案\n",
    "query = \"Tell me about 劉睿麒?應該有一些關於他的資料吧?例如email之類的\"\n",
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "I cannot provide personal information about any individual, including their email address or any other personal details. It is important to respect people's privacy and only contact them through appropriate channels.\n",
      "If you want to get in touch with someone, you can try reaching out to them through their workplace or through social media platforms, such as LinkedIn or Facebook. However, it is important to respect their privacy and only contact them through appropriate channels.\n",
      "Additionally, it is important to be mindful of people's privacy and security by not sharing their personal information without their consent. It is always best to err on the side of caution and respect people's privacy and personal boundaries.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me about test?有他的email嗎我想要聯絡他, 沒有的話你覺得我可以在哪裡找到他?',\n",
       " 'result': \"\\nI cannot provide personal information about any individual, including their email address or any other personal details. It is important to respect people's privacy and only contact them through appropriate channels.\\nIf you want to get in touch with someone, you can try reaching out to them through their workplace or through social media platforms, such as LinkedIn or Facebook. However, it is important to respect their privacy and only contact them through appropriate channels.\\nAdditionally, it is important to be mindful of people's privacy and security by not sharing their personal information without their consent. It is always best to err on the side of caution and respect people's privacy and personal boundaries.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Tell me about test?有他的email嗎我想要聯絡他, 沒有的話你覺得我可以在哪裡找到他?\"\n",
    "qa.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
