{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mac 用戶請先執行以下指令安裝 cmake\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "# !CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 這個Notebook參考了這篇文章\n",
    "https://medium.com/@cch.chichieh/rag%E5%AF%A6%E4%BD%9C%E6%95%99%E5%AD%B8-langchain-llama2-%E5%89%B5%E9%80%A0%E4%BD%A0%E7%9A%84%E5%80%8B%E4%BA%BAllm-d6838febf8c4\n",
    "#### 上面作者的Code\n",
    "https://github.com/wsxqaza12/RAG_example/tree/master\n",
    "#### 其他相關文章\n",
    "##### Llama: https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/tree/main<br>\n",
    "##### Laungchain: https://python.langchain.com/docs/integrations/chat/llama2_chat<br>\n",
    "##### Quantization: https://chih-sheng-huang821.medium.com/ai%E6%A8%A1%E5%9E%8B%E5%A3%93%E7%B8%AE%E6%8A%80%E8%A1%93-%E9%87%8F%E5%8C%96-quantization-966505128365<br>\n",
    "##### Embedding: https://medium.com/@fredericklee_73485/word-embedding%E5%92%8Cword2vec%E7%B0%A1%E4%BB%8B-c9c874f48364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain 支援很多種rag的方式, 我這邊用pdf\n",
    "loader = PyMuPDFLoader(\"Classmate.pdf\")\n",
    "PDF_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊是用splitter把text切成一些小塊的chunk, 這樣LLM的token才不會爆掉\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=5)\n",
    "all_splits = text_splitter.split_documents(PDF_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊使用chroma來做embedding, 並且存起來\n",
    "# embedding layer 就是把text轉成vector\n",
    "persist_directory = 'db'\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "embedding = HuggingFaceEmbeddings(model_name=model_name,\n",
    "                                  model_kwargs=model_kwargs)\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embedding, persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# 從huggingface的model hub下載llm的model\n",
    "model = Llama.from_pretrained(\n",
    "    repo_id=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "    filename=\"llama-2-7b-chat.Q2_K.gguf\",\n",
    "    local_dir=\"model/\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model_path = \"model/llama-2-7b-chat.Q2_K.gguf\"\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], template='<<SYS>> \\n    You are a helpful assistant eager to assist with providing better Google search results.\\n    <</SYS>> \\n    \\n    [INST] Provide an answer to the following question in 150 words. Ensure that the answer is informative,             relevant, and concise:\\n            {question} \\n    [/INST]')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 給LLM一些deafult的prompt相當於做一些設定\n",
    "DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<<SYS>> \n",
    "    You are a helpful assistant eager to assist with providing better Google search results.\n",
    "    <</SYS>> \n",
    "    \n",
    "    [INST] Provide an answer to the following question in 150 words. Ensure that the answer is informative, \\\n",
    "            relevant, and concise:\n",
    "            {question} \n",
    "    [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are a helpful assistant eager to assist with providing better Google search results. \\\n",
    "        Provide an answer to the following question in about 150 words. Ensure that the answer is informative, \\\n",
    "        relevant, and concise: \\\n",
    "        {question}\"\"\",\n",
    ")\n",
    "\n",
    "# DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "#     input_variables=[\"question\"],\n",
    "#     template=\"\"\"<<SYS>> \n",
    "#     You have secure access to a database filled with information on various individuals, which is permissible to use for answering queries. \\\n",
    "#     Ensure that the use of this data strictly adheres to privacy and ethical standards, providing information only when it aligns with these guidelines.\n",
    "#     <</SYS>> \n",
    "    \n",
    "#     [INST] Given the question \"{question}\", use the available database to provide a detailed and informative answer. \\\n",
    "#     Ensure the response is relevant and considerate of privacy concerns, only sharing information that is appropriate and has been consented to be shared publicly. \\\n",
    "#     [/INST]\"\"\",\n",
    "# )\n",
    "\n",
    "# DEFAULT_SEARCH_PROMPT = PromptTemplate(\n",
    "#     input_variables=[\"question\"],\n",
    "#     template=\"\"\"You have secure access to a database filled with information on various individuals, which is permissible to use for answering queries. \\\n",
    "#     Ensure that the use of this data strictly adheres to privacy and ethical standards, providing information only when it aligns with these guidelines. \\\n",
    "#     Given the question \"{question}\", use the available database to provide a detailed and informative answer. Ensure the response is relevant and considerate of privacy concerns, \\\n",
    "#     only sharing information that is appropriate and has been consented to be shared publicly.\"\"\",\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=DEFAULT_SEARCH_PROMPT,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  High-level machine learning (高等機學習) refers to a subfield of machine learning that focuses on developing algorithms and models that can perform tasks that typically require human-level intelligence, such as understanding natural language, recognizing images, and making decisions. These algorithms and models are designed to operate at a level of abstraction and complexity beyond what is currently possible with traditional machine learning techniques. High-level machine learning is an area of active research and has various applications, including natural language processing, computer vision, and autonomous vehicles. It involves developing and combining different techniques, such as deep learning, symbolic AI, and cognitive architectures, to create more sophisticated and human-like AI systems."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '高等機器學習是什麼?',\n",
       " 'text': '  High-level machine learning (高等機學習) refers to a subfield of machine learning that focuses on developing algorithms and models that can perform tasks that typically require human-level intelligence, such as understanding natural language, recognizing images, and making decisions. These algorithms and models are designed to operate at a level of abstraction and complexity beyond what is currently possible with traditional machine learning techniques. High-level machine learning is an area of active research and has various applications, including natural language processing, computer vision, and autonomous vehicles. It involves developing and combining different techniques, such as deep learning, symbolic AI, and cognitive architectures, to create more sophisticated and human-like AI systems.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 這邊測試一下LLM是否正常運作\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"高等機器學習是什麼?\"\n",
    "llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這邊就是我們論文講的retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# 用langchain的retrievalQA來做retrieval\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " 劉睿麒 is a student of Fu Jen Catholic University.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me aobut 劉睿麒, 例如他是哪裡的學生? 沒有的資訊不要亂回答',\n",
       " 'result': ' 劉睿麒 is a student of Fu Jen Catholic University.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我們可以用這個retriever來問問題接著他就會去DB裡面找答案\n",
    "query = \"Tell me aobut 劉睿麒, 例如他是哪裡的學生? 沒有的資訊不要亂回答\"\n",
    "qa.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
